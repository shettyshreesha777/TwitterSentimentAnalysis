{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a57840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Amount of Positive tweets: 0.0\n",
      "Amount of Negative tweets: 0.0\n",
      "Amount of Neutral tweets: 0.0\n",
      "['negative', 'neutral', 'positive']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9748/3615670944.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcountplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Emotion\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0memotion_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'magma'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtextdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m             )\n\u001b[0;32m     45\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py\u001b[0m in \u001b[0;36mcountplot\u001b[1;34m(x, y, hue, data, order, hue_order, orient, color, palette, saturation, dodge, ax, **kwargs)\u001b[0m\n\u001b[0;32m   3596\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot pass values for both `x` and `y`\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3598\u001b[1;33m     plotter = _CountPlotter(\n\u001b[0m\u001b[0;32m   3599\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhue_order\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3600\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mci\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, seed, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge)\u001b[0m\n\u001b[0;32m   1584\u001b[0m         self.establish_variables(x, y, hue, data, orient,\n\u001b[0;32m   1585\u001b[0m                                  order, hue_order, units)\n\u001b[1;32m-> 1586\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestablish_colors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1587\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimate_statistic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mci\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\seaborn\\categorical.py\u001b[0m in \u001b[0;36mestablish_colors\u001b[1;34m(self, color, palette, saturation)\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[1;31m# Determine the gray color to use for the lines framing the plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[0mlight_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcolorsys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrgb_to_hls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrgb_colors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         \u001b[0mlum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlight_vals\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m.6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m         \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrgb2hex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x504 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import tweepy\n",
    "import configparser\n",
    "import datetime\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "api_key = 'rgGB64oTvU6WMCFNX7Pyxue5r'\n",
    "api_key_secret = 'yGhVDdCWIVhps9IhAofymshhIbQuC9gmwAR86mQXk7dWiWl42X'\n",
    "\n",
    "access_token = '1520724296070758400-M1OCv9vcaELtojfbHZDDJRhLmODtQG'\n",
    "access_token_secret = 'RjRAZf7YdvNOypq7yG0673l1S2N5IWIRks2pmgKgqLMiv'\n",
    "\n",
    "# authentication\n",
    "auth_handler = tweepy.OAuthHandler(api_key, api_key_secret)\n",
    "auth_handler.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth_handler)\n",
    "\n",
    "start_date = datetime.datetime(2022, 4, 14)\n",
    "end_date = datetime.datetime(2022, 4, 20)\n",
    "term=\"twittersold\"\n",
    "limit=100\n",
    "public_tweets = tweepy.Cursor(api.search_tweets,q=term).items(limit)\n",
    "\n",
    "polarity = 0.0\n",
    "positive = 0.0\n",
    "negative = 0.0\n",
    "neutral = 0.0\n",
    "# create dataframe\n",
    "columns = ['User', 'Tweet', 'class']\n",
    "data = []\n",
    "\n",
    "for tweet in public_tweets:\n",
    "    analysis = TextBlob(tweet.text) \n",
    "    tweet_polarity = analysis.polarity \n",
    "    if tweet_polarity > 0:\n",
    "        positive += 1 \n",
    "        data.append([tweet.user.screen_name, tweet.text,'POSITIVE'])\n",
    "        #print('pos')\n",
    "    elif tweet_polarity < 0 :\n",
    "        negative +=1\n",
    "        data.append([tweet.user.screen_name, tweet.text,'NEGATIVE'])\n",
    "        #print('neg')\n",
    "    else:\n",
    "        neutral +=1\n",
    "        data.append([tweet.user.screen_name, tweet.text,'NEUTRAL'])\n",
    "        #print('neu')\n",
    "    polarity += analysis.polarity\n",
    "\n",
    "print(polarity)\n",
    "print(f'Amount of Positive tweets: {positive}')\n",
    "print(f'Amount of Negative tweets: {negative}')\n",
    "print(f'Amount of Neutral tweets: {neutral}')\n",
    "\n",
    "\n",
    "tw_list= pd.DataFrame(data, columns=columns)\n",
    "#Cleaning Text (RT, Punctuation etc)\n",
    "#Creating new dataframe and new features\n",
    "#tw_list['Tweet'] = tw_list[0]\n",
    "#Removing RT, Punctuation etc\n",
    "remove_rt = lambda x: re.sub('RT @[\\w]*: ',\" \",x)\n",
    "rt = lambda x: re.sub(\"(@[A-Za-z0–9]+)|(\\w+:\\/\\/\\S+)\",\" \",x)\n",
    "tw_list['Tweet'] = tw_list.Tweet.map(remove_rt).map(rt)\n",
    "tw_list['Tweet'] = tw_list.Tweet.str.lower()\n",
    "#tw_list.to_csv('tweets3.csv') \n",
    "\n",
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\").to(device)\n",
    "import urllib\n",
    "import csv\n",
    "import numpy as np\n",
    "labels=[]\n",
    "task = 'sentiment'\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "print(labels)\n",
    "\n",
    "from scipy.special import softmax\n",
    "BATCH_SIZE = 10\n",
    "sentiment_df=tw_list\n",
    "scores_all = np.empty((0,len(labels)))\n",
    "text_all = sentiment_df['Tweet'].to_list()\n",
    "n = len(text_all)\n",
    "with torch.no_grad():\n",
    "    for start_idx in range(0, n, BATCH_SIZE):\n",
    "        end_idx = min(start_idx+BATCH_SIZE, n)\n",
    "        encoded_input = tokenizer(text_all[start_idx:end_idx], return_tensors='pt', padding=True, max_length=512,truncation=True).to(device)\n",
    "        output = model(**encoded_input)\n",
    "        scores = output[0].detach().cpu().numpy()\n",
    "        scores = softmax(scores, axis=1)\n",
    "        scores_all = np.concatenate((scores_all, scores), axis=0)\n",
    "        del encoded_input, output, scores\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "\n",
    "sentiment_df[labels] = pd.DataFrame(scores_all, columns=labels)\n",
    "#print(sentiment_df)\n",
    "\n",
    "from textblob import TextBlob\n",
    "def get_sentiment(tweet):\n",
    "    sentiment = TextBlob(tweet).sentiment\n",
    "    return sentiment.polarity, sentiment.subjectivity\n",
    "\n",
    "sentiment_df['sentiment'] = sentiment_df['Tweet'].apply(get_sentiment)\n",
    "sentiment_df['polarity'] = sentiment_df['sentiment'].apply(lambda x:x[0])\n",
    "sentiment_df['subjectivity'] = sentiment_df['sentiment'].apply(lambda x:x[0])\n",
    "sentiment_df.drop('sentiment', axis=1, inplace=True)\n",
    "\n",
    "emotion_tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-emotion\")\n",
    "emotion_model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-emotion\").to(device)\n",
    "task='emotion'\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "scores_all = np.empty((0,len(labels)))\n",
    "text_all = sentiment_df['Tweet'].to_list()\n",
    "n = len(text_all)\n",
    "with torch.no_grad():\n",
    "    for start_idx in range(0, n, BATCH_SIZE):\n",
    "        end_idx = min(start_idx+BATCH_SIZE, n)\n",
    "        encoded_input = tokenizer(text_all[start_idx:end_idx], return_tensors='pt', padding=True, max_length=512,truncation=True).to(device)\n",
    "        output = emotion_model(**encoded_input)\n",
    "        scores = output[0].detach().cpu().numpy()\n",
    "        scores = softmax(scores, axis=1)\n",
    "        scores_all = np.concatenate((scores_all, scores), axis=0)\n",
    "        del encoded_input, output, scores\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "sentiment_df[labels] = pd.DataFrame(scores_all, columns=labels)\n",
    "#print(sentiment_df)\n",
    "emotion_df=sentiment_df\n",
    "emotion_df.drop(['negative','positive','neutral','polarity','subjectivity'], axis=1, inplace=True)\n",
    "emotion_df.insert(7, \"Emotion\", '')\n",
    "for i in range(len(emotion_df)):\n",
    "  if emotion_df['anger'][i] > emotion_df['joy'][i] and emotion_df['anger'][i] > emotion_df['optimism'][i] and emotion_df['anger'][i] > emotion_df['sadness'][i]:\n",
    "    emotion_df['Emotion'][i] = 'anger'\n",
    "  elif emotion_df['joy'][i] > emotion_df['anger'][i] and emotion_df['joy'][i] > emotion_df['optimism'][i] and emotion_df['joy'][i] > emotion_df['sadness'][i]:\n",
    "    emotion_df['Emotion'][i]= 'joy'\n",
    "  elif emotion_df['optimism'][i] > emotion_df['anger'][i] and emotion_df['optimism'][i] > emotion_df['joy'][i] and emotion_df['optimism'][i] > emotion_df['sadness'][i]:\n",
    "    emotion_df['Emotion'][i]= 'optimism'\n",
    "  else:\n",
    "    emotion_df['Emotion'][i] = 'sadness'\n",
    "\n",
    "emotion_df.drop(['anger','joy','optimism','sadness'], axis=1, inplace=True)\n",
    "#graph\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize = (8,7))\n",
    "sns.countplot(x=\"Emotion\", data=emotion_df, palette='magma')\n",
    "\n",
    "def preprocess(textdata):\n",
    "    processedText = []\n",
    "    # Create Lemmatizer and Stemmer.\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    for tweet in textdata:\n",
    "        tweetwords = ''\n",
    "        for word in tweet.split():\n",
    "             # Checking if the word is a stopword.\n",
    "             #if word not in stopwordlist:\n",
    "             if len(word)>1:\n",
    "                # Lemmatizing the word.\n",
    "                word = wordLemm.lemmatize(word)\n",
    "                tweetwords += (word+' ')\n",
    "            \n",
    "        processedText.append(tweetwords)\n",
    "        \n",
    "    return processedText\n",
    "\n",
    "#Accuracy and Confusion Matrix\n",
    "processedtext = preprocess(emotion_df['Tweet'])\n",
    "dataset = emotion_df[['Emotion','Tweet']]\n",
    "text, emotion = list(dataset['Tweet']), list(dataset['Emotion'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(processedtext, emotion,test_size = 0.05, random_state = 0)\n",
    "print(f'Data Split done.')\n",
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectoriser.fit(X_train)\n",
    "print(f'Vectoriser fitted.')\n",
    "print('No. of feature_words: ', len(vectoriser.get_feature_names()))\n",
    "X_train = vectoriser.transform(X_train)\n",
    "X_test  = vectoriser.transform(X_test)\n",
    "print(f'Data Transformed.')\n",
    "\n",
    "#EVALUATION\n",
    "    #Bernoulli\n",
    "BNBmodel = BernoulliNB(alpha = 2)\n",
    "BNBmodel.fit(X_train, y_train)\n",
    "ys_predict = BNBmodel.predict(X_test)\n",
    "#Display the outcome of classification\n",
    "print('\\n\\nBernoulli Classification Report: \\n',metrics.classification_report(y_test, ys_predict))\n",
    "print('Confusion Matrix: \\n',metrics.confusion_matrix(y_test, ys_predict))\n",
    "print('Accuracy Score: \\n',metrics.accuracy_score(y_test, ys_predict))\n",
    "\n",
    "    #KNN\n",
    "KNNModel = KNeighborsClassifier(n_neighbors=5)\n",
    "KNNModel.fit(X_train, y_train)\n",
    "ys_predict = KNNModel.predict(X_test)\n",
    "#Display the outcome of classification\n",
    "print('\\n\\nKNN Classification Report: \\n',metrics.classification_report(y_test, ys_predict))\n",
    "print('Confusion Matrix: \\n',metrics.confusion_matrix(y_test, ys_predict))\n",
    "print('Accuracy Score: \\n',metrics.accuracy_score(y_test, ys_predict))\n",
    "\n",
    "    #Decision Tree\n",
    "DTCModel = DecisionTreeClassifier(random_state=0)\n",
    "DTCModel.fit(X_train, y_train)\n",
    "ys_predict = DTCModel.predict(X_test)\n",
    "#Display the outcome of classification\n",
    "print('\\n\\nDecision Tree Classification Report: \\n',metrics.classification_report(y_test, ys_predict))\n",
    "print('Confusion Matrix: \\n',metrics.confusion_matrix(y_test, ys_predict))\n",
    "print('Accuracy Score: \\n',metrics.accuracy_score(y_test, ys_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf9fb82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3624a12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304538a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
